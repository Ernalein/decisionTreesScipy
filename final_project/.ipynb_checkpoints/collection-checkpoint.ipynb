{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df24fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45fc03",
   "metadata": {},
   "source": [
    "## Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a76456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    The class Node has all the getter and setter functions that are necessary for a tree. \n",
    "\n",
    "    Furthermore it has additional setter and getter for the attributes, values and classification\n",
    "    and a function for printing the tree. It also has a variable to indicate whether or not the \n",
    "    attribute had continous variables.\n",
    "\n",
    "    The decision tree knows its attributes and the corresponding values in the following way:\n",
    "    The parent node knows the attribute according to which the data was split.\n",
    "    The children know the values they take according to the attribute of the parent node.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, parent = None, attribute = None, classification = None, value = None, valueIsContinuous = False, target = None):\n",
    "        self.children = []  \n",
    "        self.parent = parent\n",
    "        self.attribute = attribute\n",
    "        self.classification = classification\n",
    "        self.value = value\n",
    "        self.valueIsContinuous = valueIsContinuous\n",
    "        self.target = target\n",
    "\n",
    "    \n",
    "    # typical setter and getter for tree structure \n",
    "    def setChild(self, node):\n",
    "        self.children.append(node)\n",
    "        \n",
    "    def setParent(self, node):\n",
    "        if (self.parent is not None):\n",
    "            self.parent.children.remove(self)\n",
    "        self.parent = node\n",
    "\n",
    "    def getChildren(self):\n",
    "        return self.children\n",
    "    \n",
    "    def getParent(self):\n",
    "        return self.parent\n",
    "    \n",
    "\n",
    "    # functions to check whether node is a leaf or root node\n",
    "    def isLeaf(self):\n",
    "        if len(self.children) > 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def isRoot(self):\n",
    "        if (self.parent is None):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # setter and getter for attributes, values and classification \n",
    "\n",
    "    def setAttribute(self, attribute):\n",
    "        self.attribute = attribute\n",
    "    \n",
    "    def getAttribute(self):\n",
    "        return self.attribute\n",
    "    \n",
    "    def setClassification(self, classification):\n",
    "        self.classification = classification\n",
    "    \n",
    "    def getClassification(self):\n",
    "        return self.classification\n",
    "    \n",
    "    def setValue(self,value):\n",
    "        self.value = value\n",
    "\n",
    "    def getValue(self):\n",
    "        return self.value\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------\n",
    "    # function to print the tree\n",
    "    def printTree(self, level = 0):\n",
    "        tab = \"    \"\n",
    "        if level == 0:\n",
    "            print(\"Decision tree:\")\n",
    "        if self.isLeaf():\n",
    "            print(tab*level, \"classification: \", self.target, \" = \" , self.classification)\n",
    "        else:\n",
    "            for child in self.children:\n",
    "                # printing intervals\n",
    "                if child.valueIsContinuous:\n",
    "                    interval = \"\" \n",
    "                    if child.value[0] == np.NINF:\n",
    "                        interval = f\"smaller then {child.value[1]}\"\n",
    "                    elif child.value[1] == np.PINF:\n",
    "                        interval = f\"bigger then {child.value[0]}\"\n",
    "                    else:\n",
    "                        interval = f\"between {child.value[0]} and {child.value[1]}\"\n",
    "                    print(tab*level, self.attribute, \": \", interval)\n",
    "                \n",
    "                # printing discrete values\n",
    "                else:\n",
    "                    print(tab*level, self.attribute, \": \", child.value)\n",
    "                \n",
    "                # traverse deeper into the tree\n",
    "                child.printTree(level + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec05a79",
   "metadata": {},
   "source": [
    "## Train_Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8edf112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_data:\n",
    "    '''\n",
    "    The class Train_data has all important functions for the ID3 Algorithm with continous \n",
    "    and discrete variables:\n",
    "\n",
    "    The functions isContinous(), getBoundaries(), setBoundaries() and replaceContinous() are \n",
    "    responsible for handeling continuous variables:\n",
    "    isContinous() detects when an attribute column consists of continuous values which is \n",
    "    the case if there are more than 10 different numerical values. \n",
    "    getBoundaries() chooses the boundary tuples representing intervals according to the classifications. \n",
    "    When this returns more than 10 different intervals setBoundaries() sets 10 evenly sized \n",
    "    intervals independent of classification instead.\n",
    "    With replaceContinous() the values is the continous attribute column are then replaced with \n",
    "    the boundary tuple in which they lie inbetween. \n",
    "    The fuction sortIntervals() sorts the resulting intervals.\n",
    "\n",
    "    The functions entropy(), informationGain(), gainRatio() and chooseAttribute() are for choosing\n",
    "    an attribute column with the highest gain Ratio. Gain Ratio is choosen instead of Information Gain,\n",
    "    so that for many valued attributes those with few values are prefered.\n",
    "\n",
    "    The functions classify() and id3() are for building the tree. Where classify() returns the \n",
    "    current classification of the node and id3() is the ID3 algorithm for training a decision tree.\n",
    "\n",
    "    The class only takes a pd.Dataframe as data, string as target and a list of strings \n",
    "    as attributes. \n",
    "    Furthermore if wished the maximal recursion depth of the ID3 algorithm can be set via \n",
    "    max_recursion.\n",
    "    '''\n",
    "    def __init__(self, data, target, attributes, node:Node = None, recursion_depth = 0, continuous_splitting = 0.1,  max_recursion = np.PINF):\n",
    "        \n",
    "        self.data = data\n",
    "        if not isinstance(self.data, pd.DataFrame):\n",
    "            raise TypeError(\"Data has to be a Pandas Dataframe\")\n",
    "        \n",
    "        self.target = target\n",
    "        if not isinstance(self.target, str):\n",
    "            raise TypeError(\"Taget has to be of type string\")\n",
    "        \n",
    "        self.attributes = attributes\n",
    "        if not isinstance(attributes, list):\n",
    "            raise TypeError(\"Attributes have to have structure list\")\n",
    "            \n",
    "        for attribute in self.attributes:\n",
    "            if not isinstance(attribute, str):\n",
    "                raise TypeError(\"Attributes have to be of type string\")\n",
    "\n",
    "        self.node = node\n",
    "        self.continuous_splitting = continuous_splitting\n",
    "        self.recursion_depth = recursion_depth\n",
    "        self.max_recursion = max_recursion\n",
    "    \n",
    "    ######################################\n",
    "    ## methods for continuous variables ##\n",
    "    ######################################\n",
    "    \n",
    "    def isContinuous(self, values):\n",
    "        # checks is variable is a continuous variable\n",
    "        # (it is continuous if it has more than 10 different values and is a numericla scalar)\n",
    "        if len(values) > 10:\n",
    "            if isinstance(list(values)[5], int) or isinstance(list(values)[0], float):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def getBoundaries(self, tColumn, aColumn):\n",
    "        # by looking at the target column and the attribute column the\n",
    "        # function decides on decision boundaries in a continuous varibale, where classification changes\n",
    "        # aColumn -> attribute column with the continuous values\n",
    "        # tColumn -> target column with the classification\n",
    "        \n",
    "        # 1) sort the two columns by attribute values\n",
    "        columns = pd.DataFrame(data={\"a\":list(aColumn), \"t\":list(tColumn)}).sort_values(by=\"a\")\n",
    "        columns.index = range(len(columns))\n",
    "\n",
    "        # 2) find decision boundaries where classification changes\n",
    "        leftBound = np.NINF # first interval has negative infinity as left boundary\n",
    "        rightBound = None\n",
    "        boundaries = []\n",
    "        currentClass = columns[\"t\"][0]\n",
    "        \n",
    "        for i in range(len(columns)):\n",
    "            \n",
    "            # when classification changes\n",
    "            if(columns[\"t\"][i] != currentClass):\n",
    "                currentClass = columns[\"t\"][i]\n",
    "                \n",
    "                # get the value in the middel of the values where classification changes\n",
    "                beforeSwitch = columns[\"a\"][i-1]\n",
    "                afterSwitch = columns[\"a\"][i]\n",
    "                rightBound = (beforeSwitch + afterSwitch) / 2\n",
    "\n",
    "                # safe the tupple of two boundaries, \n",
    "                # represents an interval with a uniform classification\n",
    "                boundaries.append((leftBound, rightBound))\n",
    "                leftBound = rightBound\n",
    "        \n",
    "        # last interval has positive infinity as right boundary\n",
    "        boundaries.append((leftBound, np.PINF))\n",
    "        \n",
    "        # if the getBoundaries function returns more then 10 intervals\n",
    "        # set intervals independent of classification\n",
    "        if len(boundaries) > 10:\n",
    "            return self.setBoundaries(aColumn)\n",
    "        \n",
    "        return boundaries\n",
    "    \n",
    "    def setBoundaries(self, aColumn):\n",
    "        # if the getBoundaries function returns more then 10 intervals\n",
    "        # sets 10 eaqually sized intervals indipendent of classification\n",
    "        \n",
    "        # calculate size of intervals\n",
    "        maximum = np.max(aColumn)\n",
    "        minimum = np.min(aColumn)\n",
    "        stepsize = (maximum - minimum)/ 10\n",
    "        boundaries = []\n",
    "        \n",
    "        # make a tupel for each interval\n",
    "        leftBound = np.NINF\n",
    "        rightBound = minimum + stepsize\n",
    "        for i in range(9):\n",
    "            boundaries.append((leftBound, rightBound))\n",
    "            leftBound = rightBound\n",
    "            rightBound = leftBound + stepsize\n",
    "        boundaries.append((leftBound, np.PINF))\n",
    "        \n",
    "        return boundaries\n",
    "        \n",
    "    \n",
    "    def replaceContinuous(self, boundaries, aColumn):\n",
    "        # replaces the continuous values of an attribute by the\n",
    "        # tuples that represent an interval\n",
    "        \n",
    "        newAColumn = []\n",
    "        for value in aColumn:\n",
    "            # find the interval that includes the value\n",
    "            foundInterval = False\n",
    "            for l, r in boundaries:\n",
    "                if value >= l and value < r:\n",
    "                    newAColumn.append((l, r))\n",
    "                    foundInterval = True\n",
    "                    break\n",
    "            if foundInterval == False:\n",
    "                raise TypeError(\"could not find and interval for \", value)\n",
    "        \n",
    "        return pd.Series(newAColumn)\n",
    "    \n",
    "\n",
    "    def sortIntervals(self, unsortedV):\n",
    "        # sortes the intervals according to their value\n",
    "        sortedV = []\n",
    "        leftBound = np.NINF\n",
    "        for value in unsortedV:\n",
    "            if value[0] == leftBound:\n",
    "                leftBound = value[1]\n",
    "                sortedV.append(value)\n",
    "        return sortedV\n",
    "    \n",
    "    #######################################\n",
    "    ## methods for choosing an attribute ##\n",
    "    #######################################\n",
    "    \n",
    "    \n",
    "    def entropy(self, targetColumn):\n",
    "        values = set(targetColumn)\n",
    "        entropySum = 0\n",
    "\n",
    "        # Itterates through all values and calculates the sum of the entropies of the values\n",
    "        for value in values:\n",
    "            p = list(targetColumn).count(value) / len(targetColumn)\n",
    "            entropySum = entropySum + (- p * np.log(p))\n",
    "\n",
    "        return entropySum\n",
    "    \n",
    "\n",
    "    def informationGain(self, attributeColumn, values):\n",
    "        # calculates the informationGain\n",
    "        gainSum = 0\n",
    "        for value in values:\n",
    "            mask = lambda aColumn, value :(row == value for row in aColumn) \n",
    "            subsetData = self.data.iloc[mask(attributeColumn, value),:]\n",
    "            subsetTargetColumn = subsetData[self.target]\n",
    "            # claculate entropy and normalize by size of subsets\n",
    "            gainSum = gainSum + (len(subsetData)/ len(self.data)) * self.entropy(subsetTargetColumn)\n",
    "\n",
    "        # substract summed and weighted entropy of subsets from entropy of whole set\n",
    "        infoGain = self.entropy(self.data.loc[:, self.target]) - gainSum\n",
    "\n",
    "        return infoGain\n",
    "\n",
    "    def gainRatio(self, attributeColumn, values):\n",
    "        # calculating the Gain Ratio instead of the InforamtionGain\n",
    "        # to prefer attributes with few values\n",
    "        infoGain = self.informationGain(attributeColumn, values)\n",
    "        \n",
    "        splitInfo = 0.0\n",
    "        for value in values:\n",
    "            subset = attributeColumn[attributeColumn == value]\n",
    "            # proportion of subset size and whole set size\n",
    "            s = len(subset) / len(attributeColumn)\n",
    "            if s != 0.0:\n",
    "                splitInfo = splitInfo + ((- s) * np.log(s))\n",
    "        \n",
    "        # to avoid dividing by zero\n",
    "        if splitInfo == 0:\n",
    "            splitInfo = infoGain\n",
    "            if infoGain == 0:\n",
    "                return 0\n",
    "            \n",
    "        return infoGain / splitInfo\n",
    "        \n",
    "\n",
    "    def chooseAttribute(self):\n",
    "        # chooses an attribute that maximises GainRatio\n",
    "        maxGain= 0\n",
    "        maxAttribute = \"\"\n",
    "\n",
    "        # calculate Gain Ratio for each attribute\n",
    "        for attribute in self.attributes:\n",
    "            attributeColumn = self.data[attribute]\n",
    "            values = set(attributeColumn)\n",
    "            gain = 0\n",
    "\n",
    "            # replace the continuous values in attributeColumn by Intervals\n",
    "            if self.isContinuous(values):\n",
    "                targetColumn = self.data[self.target]\n",
    "                boundaries = self.getBoundaries(targetColumn, attributeColumn)\n",
    "                attributeColumn = self.replaceContinuous(boundaries, attributeColumn)\n",
    "                values = set(attributeColumn)            \n",
    "                \n",
    "            # calculate gainRatio\n",
    "            gain = self.gainRatio(attributeColumn, values)\n",
    "\n",
    "            # store attribute with highest information gain\n",
    "            if gain >= maxGain:\n",
    "                maxGain = gain\n",
    "                maxAttribute = attribute\n",
    "\n",
    "        # choose attribute with highest Information Gain\n",
    "        return maxAttribute\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    ## methods for building the decision tree ##\n",
    "    ############################################\n",
    "    \n",
    "    def classify(self):\n",
    "        # returns the most commen classification of the dataset\n",
    "        targetColumn = self.data.loc[:, self.target]\n",
    "        values = set(targetColumn)\n",
    "        maxClass = 0  # highest number of values\n",
    "        classification = \"\" # classification of most common value\n",
    "\n",
    "        for value in values:\n",
    "            # check if calssification value is more common then other classification values\n",
    "            if list(targetColumn).count(value) > maxClass:\n",
    "                maxClass = list(targetColumn).count(value)\n",
    "                classification = value\n",
    "\n",
    "        return classification\n",
    "    \n",
    "\n",
    "    def id3(self):\n",
    "\n",
    "        # BASE CASES:\n",
    "\n",
    "        # 1) all instances have same target value: \n",
    "        #    -> create a leaf node with target value as classification\n",
    "        if (self.data[self.target].nunique() == 1):\n",
    "            self.node.setClassification(self.data[self.target].iloc[0])\n",
    "            return \n",
    "\n",
    "        # 2) out of desciptive features (list of attributes to choose is empty) \n",
    "        #    -> create a leaf node with majority of target values as classification\n",
    "        if (not self.attributes):\n",
    "            self.node.setClassification(self.classify())\n",
    "            return\n",
    "\n",
    "        # 3) no instances left in dataset \n",
    "        #    -> take majority of target values of the parent node as classification\n",
    "        if (self.data is None):\n",
    "            parent = self.node.getParent()\n",
    "            self.node.setClassification(parent.getClassification())\n",
    "            return\n",
    "\n",
    "        # 4) when recursion depth is limited and limit is reached:\n",
    "        #    -> no further splitting of the data, \n",
    "        #       current node is leaf node with majority of target values as classification\n",
    "        if self.recursion_depth >= self.max_recursion:\n",
    "            self.node.setClassification(self.classify())\n",
    "            return\n",
    "\n",
    "\n",
    "        # RECURSIVE CASE:\n",
    "\n",
    "        # choose attribute with highest explainatory power\n",
    "        attribute = self.chooseAttribute()\n",
    "\n",
    "        # set the attribute and the classification of the current node\n",
    "        self.node.setAttribute(attribute)\n",
    "        self.node.setClassification(self.classify())\n",
    "\n",
    "        # split data according to attribute\n",
    "        attributeColumn = self.data.loc[:, attribute]\n",
    "        values = set(attributeColumn)\n",
    "\n",
    "        # make a new list of attributes without the current attribute\n",
    "        new_attributes = self.attributes\n",
    "        new_attributes.remove(attribute)\n",
    "        \n",
    "        # add 1 to the recursion_depth\n",
    "        recursion_depth = self.recursion_depth + 1\n",
    "\n",
    "        # when chosen attribute is a continuous variable:\n",
    "        # replace the continous values with corresponding boundary tuples\n",
    "        valueIsContinuous = False\n",
    "        if self.isContinuous(values):\n",
    "            targetColumn = self.data[self.target]\n",
    "            boundaries = self.getBoundaries(targetColumn, attributeColumn)\n",
    "            attributeColumn = self.replaceContinuous(boundaries, attributeColumn)\n",
    "            values = set(attributeColumn)\n",
    "            values = self.sortIntervals(values)\n",
    "            valueIsContinuous = True\n",
    "\n",
    "        \n",
    "        # create leaf node for each attribute value\n",
    "        for value in values:\n",
    "            # get the subset determined by the attribute value\n",
    "            mask = lambda aColumn, value :(row == value for row in aColumn) \n",
    "            subsetData = self.data.iloc[mask(attributeColumn, value),:]\n",
    "            # create a node in the tree\n",
    "            childNode = Node(parent=self.node, value=value, valueIsContinuous=valueIsContinuous, target=self.target)\n",
    "            self.node.setChild(childNode)\n",
    "            # train the node with the data subset\n",
    "            subset = Train_data(data=subsetData, \n",
    "                                target=self.target, \n",
    "                                attributes=new_attributes, \n",
    "                                node=childNode, \n",
    "                                recursion_depth=recursion_depth, \n",
    "                                max_recursion = self.max_recursion)\n",
    "\n",
    "            # recursive call on all partitions                    \n",
    "            subset.id3() \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fc2ed",
   "metadata": {},
   "source": [
    "## Test_data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "410057a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_data:\n",
    "\n",
    "    '''\n",
    "    The class Test_data tests the accuracy of a decisiontree with the testing set of the data.\n",
    "\n",
    "    The function classify() is a recursive function and returns the \n",
    "    decisiontree's classification of this datapoint.\n",
    "    The function classifySet() returns the classification of the whole test data.\n",
    "    The function accuracy() is for calculating the error rate of the decisiontree. \n",
    "    The calculated error lies inbetween 0 and 1 where 1 indicates a fully right classification \n",
    "    of the testing set and 0 indicaties a fully wrong classification.\n",
    "\n",
    "    The class only takes the root node of a trained decisiontree for testing \n",
    "    and the test data has to be a pd.Dataframe.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, testData:pd.DataFrame, target, node:Node):\n",
    "        self.testData = testData\n",
    "        self.target = target\n",
    "        self.rootNode = node\n",
    "\n",
    "        # check whether node is trained:\n",
    "        if node.getAttribute() is None:\n",
    "            raise TypeError(\"node has to be part of a trained Decisiontree\")\n",
    "\n",
    "    \n",
    "    def classify(self, datapoint, node):\n",
    "        \n",
    "        # basecase 1: \n",
    "        # get the classification of the leaf node\n",
    "        if node.isLeaf() == True:\n",
    "            return node.getClassification()\n",
    "\n",
    "\n",
    "        # recursive case:\n",
    "        # traverse the tree according to attributes and values:\n",
    "        \n",
    "        # get the attribute of the current node and the attribute value of the datapoint\n",
    "        attribute = node.getAttribute()\n",
    "        dataValue = datapoint.loc[attribute]\n",
    "\n",
    "        # itterate through all children of the node\n",
    "        for child in node.getChildren():\n",
    "            cValue = child.getValue()\n",
    "            \n",
    "            # for interval values\n",
    "            if child.valueIsContinuous:\n",
    "                # check whether datapoint lies inbetween the boundaries\n",
    "                if dataValue >= cValue[0] and dataValue < cValue[1]:\n",
    "                    return self.classify(datapoint, child)\n",
    "            # for discrete values\n",
    "            elif cValue is dataValue:\n",
    "                return self.classify(datapoint, child)\n",
    "        \n",
    "        # basecase 2:\n",
    "        # if there are no children with the right value at decision node, get current classification\n",
    "        return node.getClassification()\n",
    "        \n",
    "\n",
    "    def classifySet(self):\n",
    "        classes = []\n",
    "        \n",
    "        # itterate through the test data and classify each datapoint\n",
    "        for i in range(len(self.testData)):\n",
    "            datapoint = self.testData.iloc[i]\n",
    "            classes.append(self.classify(datapoint, self.rootNode))\n",
    "        return classes\n",
    "    \n",
    "\n",
    "    def accuracy(self):\n",
    "        classes = self.classifySet()\n",
    "        targets = self.testData[self.target]\n",
    "        \n",
    "        errors = []\n",
    "        # itterates through targets and classes\n",
    "        # checks whether the classification of the decisiontree was right or not\n",
    "        for target, classification in zip(targets, classes):\n",
    "            if target == classification:\n",
    "                errors.append(1)\n",
    "            else:\n",
    "                errors.append(0)\n",
    "                \n",
    "        return np.mean(errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee8adfa",
   "metadata": {},
   "source": [
    "## prepare_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "601c2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data:pd.DataFrame, tratio = 0.1):\n",
    "    \n",
    "    # remove any Nans from Dataframe\n",
    "    data = data.dropna(how='any')\n",
    "    \n",
    "    # shuffle data\n",
    "    data = data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "    \n",
    "    # variables to return\n",
    "    testData = []\n",
    "    trainingData = []\n",
    "\n",
    "    # check whether tratio is smaller than 1\n",
    "    if tratio >= 1:\n",
    "        raise TypeError(\"tratio has to be smaller than 1\")\n",
    "    \n",
    "    # get length of dataframe\n",
    "    dataLength = data.shape[0]\n",
    "    # get chunck size:\n",
    "    chunkSize = int(dataLength * tratio) \n",
    "    # get number of chunks\n",
    "    nr_chunks = int(1/tratio)\n",
    "\n",
    "    # append data set to existing dataset\n",
    "    doubleData = data.append(data)\n",
    "\n",
    "    # itterate through doubleData, assigh a certain chunk to testing and training\n",
    "    for chunk in range(nr_chunks-1):\n",
    "        testData.append(doubleData.iloc[chunkSize*chunk: chunkSize*(chunk+1), :])\n",
    "        trainingData.append(doubleData.iloc[chunkSize*(chunk+1): chunkSize*(nr_chunks+chunk), :])\n",
    "    \n",
    "    return [testData, trainingData]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daef223",
   "metadata": {},
   "source": [
    "## Random_forest Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41562810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_forest:\n",
    "    \n",
    "    '''\n",
    "    The class Random_forest implements a reandom forest, made of multiple decision trees.\n",
    "    Each Tree is trained on a different training set, though the training sets might share some datapoints (bag of features).\n",
    "    The random forest classifies datapoints by looking at the classification from each tree \n",
    "    and then choosing the most common one.\n",
    "    \n",
    "    The Random forest is first trained with train() and then can be used to classify datapoints with \n",
    "    classifySet() or the accuracy can be calculated directly with accuracy()\n",
    "    \n",
    "    The prepare_data() function splits the data into a training and a testing set according to the testRatio and then\n",
    "    samples the training chunks from the training set according to the trainRatio.\n",
    "    \n",
    "    The variable trainRatio specifies how large a training set is, in realtion to the wohle data set, \n",
    "    testRatio does the same for the testing set. nrTrees indicates how many decision trees are making up the forest.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, data, target, trainRatio = 0.1, nrTrees = 10, testRatio = 0.3):\n",
    "\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.trees = []\n",
    "        self.trainRatio = trainRatio \n",
    "        self.nrTrees = nrTrees\n",
    "        self.testRatio = testRatio\n",
    "        self.testingSet = None\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        trainingSets = self.prepare_data()\n",
    "\n",
    "        # train a tree for each training set\n",
    "        for trainingSet in trainingSets:\n",
    "\n",
    "            attributes = list(trainingSet.columns)\n",
    "            attributes.remove(self.target)\n",
    "\n",
    "            rootNode = Node()\n",
    "            decisionTree = Train_data(data=trainingSet, target=self.target, attributes=attributes, node=rootNode, max_recursion = np.PINF)\n",
    "            decisionTree.id3()\n",
    "            self.trees.append(rootNode)\n",
    "        \n",
    "    def prepare_data(self):\n",
    "    \n",
    "        # check whether Ratios are acceptable\n",
    "        if self.testRatio >= 1:\n",
    "            raise TypeError(\"Testing ratio has to be smaller than 1\")\n",
    "            \n",
    "        if self.trainRatio >= 1:\n",
    "            raise TypeError(\"Training ratio has to be smaller than 1\")\n",
    "        \n",
    "            \n",
    "        # remove any Nans from Dataframe\n",
    "        data = self.data.dropna(how='any')\n",
    "\n",
    "        # shuffle data\n",
    "        data = data.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "        \n",
    "        # split of the testing set\n",
    "        self.testingSet = data.iloc[0:int(len(data) * self.testRatio), :]\n",
    "        trainingData = data.iloc[int(len(data) * self.testRatio):, :]\n",
    "\n",
    "        # split training Data into random sets\n",
    "        trainingSize = int(self.trainRatio * len(data))\n",
    "        trainingSets = []\n",
    "        for tree in range(self.nrTrees):\n",
    "            # suffel dataset before extracting traingset of correct size\n",
    "            trainingData = trainingData.sample(frac=1, random_state=1).reset_index(drop=True) #random shuffel\n",
    "            trainingSets.append(trainingData.iloc[:trainingSize, :])\n",
    "        \n",
    "        return trainingSets\n",
    "    \n",
    "    def classifySet(self, testingSet):\n",
    "        \n",
    "        # for each tree get classification list\n",
    "        classifications = []\n",
    "        for tree in self.trees:\n",
    "            testData = Test_data(testingSet, self.target, tree)\n",
    "            classifications.append(testData.classifySet())\n",
    "        \n",
    "        # for each datapoint get the most common classification\n",
    "        votedClass = []\n",
    "        for i in range(len(testingSet)):\n",
    "            votes = []\n",
    "            for classList in classifications:\n",
    "                votes.append(classList[i])\n",
    "            votedClass.append(max(set(votes), key = votes.count))\n",
    "\n",
    "        return votedClass\n",
    "    \n",
    "    def accuracy(self):\n",
    "        \n",
    "        # get classification of datapoints\n",
    "        votedClasses = self.classifySet(self.testingSet)\n",
    "        \n",
    "        # for each datapoint, check if classification is accurate\n",
    "        targets = self.testingSet[self.target]\n",
    "        errors = []\n",
    "        for target, classification in zip(targets, votedClasses):\n",
    "            if target == classification:\n",
    "                errors.append(True)\n",
    "            else:\n",
    "                errors.append(False)\n",
    "\n",
    "         # calculate accuracy        \n",
    "        return np.mean(errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a440a1",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1188108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_7316\\3213748873.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Decision tree trained on  penguins  Dataset for classifying  species\n",
      "accuracy score:  0.9764309764309764\n",
      "Decision tree:\n",
      " island :  Dream\n",
      "     bill_length_mm :  smaller then 45.05\n",
      "         classification:  species  =  Adelie\n",
      "     bill_length_mm :  bigger then 45.05\n",
      "         classification:  species  =  Chinstrap\n",
      " island :  Biscoe\n",
      "     flipper_length_mm :  smaller then 206.5\n",
      "         classification:  species  =  Adelie\n",
      "     flipper_length_mm :  bigger then 206.5\n",
      "         classification:  species  =  Gentoo\n",
      " island :  Torgersen\n",
      "     classification:  species  =  Adelie\n",
      "None\n",
      "A random forest trained on the same dataset reaches an accuracy of  1.0\n",
      "The random forest performs better then the best decision Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_7316\\3213748873.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Decision tree trained on  titanic  Dataset for classifying  survived\n",
      "accuracy score:  0.691358024691358\n",
      "Decision tree:\n",
      " alone :  False\n",
      "     fare :  smaller then 23.2\n",
      "         classification:  survived  =  1\n",
      " alone :  True\n",
      "     classification:  survived  =  0\n",
      "None\n",
      "A random forest trained on the same dataset reaches an accuracy of  0.7222222222222222\n",
      "The random forest performs better then the best decision Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_7316\\3213748873.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Decision tree trained on  iris  Dataset for classifying  species\n",
      "accuracy score:  0.9481481481481482\n",
      "Decision tree:\n",
      " petal_length :  smaller then 2.45\n",
      "     classification:  species  =  setosa\n",
      " petal_length :  between 2.45 and 4.9\n",
      "     classification:  species  =  versicolor\n",
      " petal_length :  bigger then 4.9\n",
      "     classification:  species  =  virginica\n",
      "None\n",
      "A random forest trained on the same dataset reaches an accuracy of  1.0\n",
      "The random forest performs better then the best decision Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_7316\\3213748873.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Decision tree trained on  tips  Dataset for classifying  sex\n",
      "accuracy score:  0.6435185185185185\n",
      "Decision tree:\n",
      " size :  2\n",
      "     total_bill :  smaller then 11.78\n",
      "         classification:  sex  =  Male\n",
      " size :  3\n",
      "     tip :  2.5\n",
      "         classification:  sex  =  Female\n",
      "     tip :  3.71\n",
      "         classification:  sex  =  Male\n",
      "     tip :  3.5\n",
      "         classification:  sex  =  Male\n",
      "     tip :  3.0\n",
      "         classification:  sex  =  Female\n",
      "     tip :  4.0\n",
      "         classification:  sex  =  Female\n",
      " size :  4\n",
      "     classification:  sex  =  Male\n",
      "None\n",
      "A random forest trained on the same dataset reaches an accuracy of  0.7083333333333334\n",
      "The random forest performs better then the best decision Tree\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# datasets and the corresponding target features for training the decision trees\n",
    "seabornToyData = [(\"penguins\", \"species\"), (\"titanic\", \"survived\"), (\"iris\", \"species\"), (\"tips\", \"sex\")]\n",
    "\n",
    "for dataset,target in seabornToyData:\n",
    "\n",
    "    # 1. load data\n",
    "    data = sns.load_dataset(dataset)\n",
    "    if dataset == \"titanic\":\n",
    "        data = data.drop(\"alive\", axis=1)\n",
    "\n",
    "    # 2. prepare training and testing chunks\n",
    "    trainingSets, testingSets = prepare_data(data, 0.1)\n",
    "\n",
    "    # 3. train a tree for each chunk of the training set -> 10 trees\n",
    "    decisionTrees = []\n",
    "    for trainingSet in trainingSets:\n",
    "\n",
    "        attributes = list(trainingSet.columns)\n",
    "        attributes.remove(target)\n",
    "\n",
    "        rootNode = Node()\n",
    "        decisionTree = Train_data(data=trainingSet, target=target, attributes=attributes, node=rootNode, max_recursion = np.PINF)\n",
    "        decisionTree.id3()\n",
    "        decisionTrees.append(rootNode)\n",
    "\n",
    "    # 4. test each tree with the corresponding testing chunk\n",
    "    accuracies = []\n",
    "    for testingSet, tree in zip(testingSets, decisionTrees):\n",
    "\n",
    "        testData = Test_data(testingSet, target, tree)\n",
    "        accuracies.append(testData.accuracy())\n",
    "\n",
    "    # 5. print tree with best accuracy score\n",
    "    maxAcc = np.max(accuracies)\n",
    "    bestTree = decisionTrees[np.argmax(accuracies)]\n",
    "    print(\"\\n\\nDecision tree trained on \", dataset ,\" Dataset for classifying \", target)\n",
    "    print(\"accuracy score: \", maxAcc)\n",
    "    print(bestTree.printTree())\n",
    "    \n",
    "    # 6. build a decision Forest with same training set size for each tree\n",
    "    forest = Random_forest(data, target, trainRatio = 0.1, nrTrees = 15, testRatio = 0.1)\n",
    "    forest.train()\n",
    "    fAcc = forest.accuracy()\n",
    "    \n",
    "    # 7. check weather decision Forest is more accurate\n",
    "    print(\"A random forest trained on the same dataset reaches an accuracy of \", fAcc)\n",
    "    if fAcc > maxAcc:\n",
    "        print(\"The random forest performs better then the best decision Tree\")\n",
    "    elif fAcc < maxAcc:\n",
    "        print(\"The random forest performs worse then the best decision Tree\")\n",
    "    else:\n",
    "        print(\"The random forest performs equal to the best decision Tree\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
