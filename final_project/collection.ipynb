{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df24fe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45fc03",
   "metadata": {},
   "source": [
    "### Node Class\n",
    "\n",
    "Tree structure is build by Node Objects. Each node object has an attribute and a classification when trained, as well as a parent node and the value associated with the parents attribute.\n",
    "The nodes are trained with train_data and also offer the printTree function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88a76456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    class Node innitialises a tree structure for a non-binary tree\n",
    "    it has the typical setter and getter methods and a method to remove a \n",
    "    child from the list of children\n",
    "    '''\n",
    "    def __init__(self, parent = None, attribute = None, classification = None, value = None, valueIsContinuous = False, target = None):\n",
    "        self.children = []  \n",
    "        self.parent = parent\n",
    "        self.attribute = attribute\n",
    "        self.classification = classification\n",
    "        self.value = value\n",
    "        self.valueIsContinuous = valueIsContinuous\n",
    "        self.target = target\n",
    "\n",
    "    \n",
    "    def setChild(self, node):\n",
    "        self.children.append(node)\n",
    "        \n",
    "    def setParent(self, node):\n",
    "        if (self.parent is not None):\n",
    "            self.parent.children.remove(self)\n",
    "        self.parent = node\n",
    "\n",
    "    def getChildren(self):\n",
    "        return self.children\n",
    "    \n",
    "    def getParent(self):\n",
    "        return self.parent\n",
    "    \n",
    "    def deleteChild(self, node):\n",
    "        if (node in self.children):\n",
    "            node.parent = None\n",
    "            self.children.remove(node)\n",
    "        else:\n",
    "            raise TypeError(\"Child not in Children\")\n",
    "        \n",
    "    def isLeaf(self):\n",
    "        if len(self.children) > 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def isRoot(self):\n",
    "        if (self.parent is None):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def setAttribute(self, attribute):\n",
    "        self.attribute = attribute\n",
    "    \n",
    "    def getAttribute(self):\n",
    "        return self.attribute\n",
    "    \n",
    "    def setClassification(self, classification):\n",
    "        self.classification = classification\n",
    "    \n",
    "    def getClassification(self):\n",
    "        return self.classification\n",
    "    \n",
    "    def setValue(self,value):\n",
    "        self.value = value\n",
    "\n",
    "    def getValue(self):\n",
    "        return self.value\n",
    "    \n",
    "\n",
    "    def printTree(self, level = 0):\n",
    "        tab = \"    \"\n",
    "        if level == 0:\n",
    "            print(\"Decision tree:\")\n",
    "        if self.isLeaf():\n",
    "            print(tab*level, \"classification: \", self.target, \" = \" , self.classification)\n",
    "        else:\n",
    "            for child in self.children:\n",
    "                # printing intervals\n",
    "                if child.valueIsContinuous:\n",
    "                    interval = \"\" \n",
    "                    if child.value[0] == np.NINF:\n",
    "                        interval = f\"smaller then {child.value[1]}\"\n",
    "                    elif child.value[1] == np.PINF:\n",
    "                        interval = f\"bigger then {child.value[0]}\"\n",
    "                    else:\n",
    "                        interval = f\"between {child.value[0]} and {child.value[1]}\"\n",
    "                    print(tab*level, self.attribute, \": \", interval)\n",
    "                \n",
    "                # printing discrete values\n",
    "                else:\n",
    "                    print(tab*level, self.attribute, \": \", child.value)\n",
    "                \n",
    "                # traverse deeper into the tree\n",
    "                child.printTree(level + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec05a79",
   "metadata": {},
   "source": [
    "### train_Data Class\n",
    "\n",
    "The class represents a data (sub)set that is used to train the (sub)tree. It has a (root) node which it is responsible to train (choose attribute, classification, and child nodes) via the id3 function.\n",
    "I can also deal with continuous variables by ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8edf112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_data:\n",
    "    '''\n",
    "    class train_data has all important functions for ID3 Algorithm:\n",
    "    it can calculate the entropy of some data, the information gain, choose an attriute.\n",
    "    within the ID3 algorithm a tree will be trained.\n",
    "    the function retrain(data) retrains a trained tree with new data.\n",
    "    '''\n",
    "    def __init__(self, data, target, attributes, node:Node = None, recursion_depth = 0, continuous_splitting = 0.1,  max_recursion = 10):\n",
    "        \n",
    "        self.data = data\n",
    "        if not isinstance(self.data, pd.DataFrame):\n",
    "            raise TypeError(\"Data has to be a Pandas Dataframe\")\n",
    "        \n",
    "        self.target = target\n",
    "        if not isinstance(self.target, str):\n",
    "            raise TypeError(\"Taget has to be of type string\")\n",
    "        \n",
    "        self.attributes = attributes\n",
    "        if not isinstance(attributes, list):\n",
    "            raise TypeError(\"Attributes have to have structure list\")\n",
    "            \n",
    "        for attribute in self.attributes:\n",
    "            if not isinstance(attribute, str):\n",
    "                raise TypeError(\"Attributes have to be of type string\")\n",
    "\n",
    "        self.node = node\n",
    "        self.continuous_splitting = continuous_splitting\n",
    "        self.recursion_depth = recursion_depth\n",
    "        self.max_recursion = max_recursion\n",
    "    \n",
    "    ######################################\n",
    "    ## methods for continuous variables ##\n",
    "    ######################################\n",
    "    \n",
    "    def is_continuous(self, values):\n",
    "        # checks is variable is a continuous variable\n",
    "        # (it is continuous if it has more than 10 different values and is a numericla scalar)\n",
    "        if len(values) > 10:\n",
    "            if isinstance(list(values)[5], int) or isinstance(list(values)[0], float):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "    def getBoundaries(self, tColumn, aColumn):\n",
    "        # by looking at the target column and the attribute column the\n",
    "        # function decides on decision boundaries in a continuous varibale, where classification changes\n",
    "        # aColumn -> attribute column with the continuous values\n",
    "        # tColumn -> target column with the classification\n",
    "        \n",
    "        # 1) sort the two columns by attribute values\n",
    "        columns = pd.DataFrame(data={\"a\":list(aColumn), \"t\":list(tColumn)}).sort_values(by=\"a\")\n",
    "        columns.index = range(len(columns))\n",
    "\n",
    "        # 2) find decision boundaries where classification changes\n",
    "        leftBound = np.NINF # first interval has negative infinity as left boundary\n",
    "        rightBound = None\n",
    "        boundaries = []\n",
    "        currentClass = columns[\"t\"][0]\n",
    "        \n",
    "        for i in range(len(columns)):\n",
    "            \n",
    "            # when classification changes\n",
    "            if(columns[\"t\"][i] != currentClass):\n",
    "                currentClass = columns[\"t\"][i]\n",
    "                \n",
    "                # get the value in the middel of the values where classification changes\n",
    "                beforeSwitch = columns[\"a\"][i-1]\n",
    "                afterSwitch = columns[\"a\"][i]\n",
    "                rightBound = (beforeSwitch + afterSwitch) / 2\n",
    "\n",
    "                # safe the tupple of two boundaries \n",
    "                # represents an interval with a uniform classification\n",
    "                boundaries.append((leftBound, rightBound))\n",
    "                leftBound = rightBound\n",
    "        \n",
    "        # last interval has negative infinity as right boundary\n",
    "        boundaries.append((leftBound, np.PINF))\n",
    "        \n",
    "        # if the getBoundaries function returns more then 10 intervals\n",
    "        # set intervals indipendent of classification\n",
    "        if len(boundaries) > 10:\n",
    "            return self.setBoundaries(aColumn)\n",
    "        \n",
    "        return boundaries\n",
    "    \n",
    "    def setBoundaries(self, aColumn):\n",
    "        # if the getBoundaries function returns more then 10 intervals\n",
    "        # sets 10 eaqually sized intervals indipendent of classification\n",
    "        \n",
    "        # calculate size of intervals\n",
    "        maximum = np.max(aColumn)\n",
    "        minimum = np.min(aColumn)\n",
    "        stepsize = (maximum - minimum)/ 10\n",
    "        boundaries = []\n",
    "        \n",
    "        # make a tupel for each interval\n",
    "        leftBound = np.NINF\n",
    "        rightBound = minimum + stepsize\n",
    "        for i in range(9):\n",
    "            boundaries.append((leftBound, rightBound))\n",
    "            leftBound = rightBound\n",
    "            rightBound = leftBound + stepsize\n",
    "        boundaries.append((leftBound, np.PINF))\n",
    "        \n",
    "        return boundaries\n",
    "        \n",
    "    \n",
    "    def replaceContinuous(self, boundaries, aColumn):\n",
    "        # replaces the continuous values of an attribute by the\n",
    "        # tuples that represent an interval\n",
    "        \n",
    "        newAColumn = []\n",
    "        for value in aColumn:\n",
    "            # find the interval that includes the value\n",
    "            foundInterval = False\n",
    "            for l, r in boundaries:\n",
    "                if value >= l and value < r:\n",
    "                    newAColumn.append((l, r))\n",
    "                    foundInterval = True\n",
    "                    break\n",
    "            if foundInterval == False:\n",
    "                raise TypeError(\"could not find and interval for \", value)\n",
    "        \n",
    "        return pd.Series(newAColumn)\n",
    "    \n",
    "    \n",
    "    #######################################\n",
    "    ## methods for choosing an attribute ##\n",
    "    #######################################\n",
    "    \n",
    "    \n",
    "    def entropy(self, targetColumn):\n",
    "        # calculates entropy \n",
    "        #targetColumn = self.data.loc[:, self.target]\n",
    "\n",
    "        values = set(targetColumn)\n",
    "        entropySum = 0\n",
    "        for value in values:\n",
    "            p = list(targetColumn).count(value) / len(targetColumn)\n",
    "            entropySum = entropySum + (- p * np.log(p))\n",
    "\n",
    "        return entropySum\n",
    "    \n",
    "\n",
    "    def informationGain(self, attributeColumn, values):\n",
    "        # calculates the informationGain\n",
    "        gainSum = 0\n",
    "        for value in values:\n",
    "            mask = lambda aColumn, value :(row == value for row in aColumn) \n",
    "            subsetData = self.data.iloc[mask(attributeColumn, value),:]\n",
    "            subsetTargetColumn = subsetData[self.target]\n",
    "            # claculate entropy and normalize by size of subsets\n",
    "            gainSum = gainSum + (len(subsetData)/ len(self.data)) * self.entropy(subsetTargetColumn)\n",
    "\n",
    "        # substract summed and weighted entropy of subsets from entropy of whole set\n",
    "        infoGain = self.entropy(self.data.loc[:, self.target]) - gainSum\n",
    "\n",
    "        return infoGain\n",
    "\n",
    "    def gainRatio(self, attributeColumn, values):\n",
    "        # calculating the Gain Ratio instead of the InforamtionGain\n",
    "        # to prefer attributes with few values\n",
    "        \n",
    "        infoGain = self.informationGain(attributeColumn, values)\n",
    "\n",
    "        splitInfo = 0.0\n",
    "        for value in values:\n",
    "            \n",
    "            subset = attributeColumn[attributeColumn == value]\n",
    "            # proportion of subset size and whole set size\n",
    "            s = len(subset) / len(attributeColumn)\n",
    "            if s != 0.0:\n",
    "                splitInfo = splitInfo + ((- s) * np.log(s))\n",
    "        \n",
    "        # to avoid dividing by zero\n",
    "        if splitInfo == 0:\n",
    "            splitInfo = infoGain\n",
    "            if infoGain == 0:\n",
    "                return 0\n",
    "            \n",
    "        return infoGain / splitInfo\n",
    "        \n",
    "\n",
    "    def chooseAttribute(self):\n",
    "        # chooses an attribute that maximises GainRatio\n",
    "        \n",
    "        maxGain= 0\n",
    "        maxAttribute = \"\"\n",
    "\n",
    "        # calculate Gain Ratio for each attribute\n",
    "        for attribute in self.attributes:\n",
    "            \n",
    "            attributeColumn = self.data[attribute]\n",
    "            values = set(attributeColumn)\n",
    "            gain = 0\n",
    "            \n",
    "            # replace the values in attributeColumn with continuous values by Intervals\n",
    "            if self.is_continuous(values):\n",
    "                targetColumn = self.data[self.target]\n",
    "                boundaries = self.getBoundaries(targetColumn, attributeColumn)\n",
    "                attributeColumn = self.replaceContinuous(boundaries, attributeColumn)\n",
    "                values = set(attributeColumn)            \n",
    "                \n",
    "            # calculate gainRatio\n",
    "            gain = self.gainRatio(attributeColumn, values)\n",
    "\n",
    "            # store attribute with highest information gain\n",
    "            if gain >= maxGain:\n",
    "                maxGain = gain\n",
    "                maxAttribute = attribute\n",
    "\n",
    "        # choose attribute with highest Information Gain\n",
    "        return maxAttribute\n",
    "\n",
    "    ############################################\n",
    "    ## methods for building the decision tree ##\n",
    "    ############################################\n",
    "    \n",
    "    def classify(self):\n",
    "        # returns the most commen classification of the dataset\n",
    "        \n",
    "        targetColumn = self.data.loc[:, self.target]\n",
    "        values = set(targetColumn)\n",
    "        maxClass = 0  # highest number of values\n",
    "        classification = \"\" # classification of most common value\n",
    "        for value in values:\n",
    "            # check if calssification value is more common then other classification values\n",
    "            if list(targetColumn).count(value) > maxClass:\n",
    "                maxClass = list(targetColumn).count(value)\n",
    "                classification = value\n",
    "\n",
    "        return classification\n",
    "    \n",
    "    def sortIntervals(self, unsortedV):\n",
    "        sortedV = []\n",
    "        leftBound = np.NINF\n",
    "        for value in unsortedV:\n",
    "            if value[0] == leftBound:\n",
    "                leftBound = value[1]\n",
    "                sortedV.append(value)\n",
    "        return sortedV\n",
    "\n",
    "    def id3(self):\n",
    "        # base cases:\n",
    "        # 1) all instances have same target value -> leaf node with target value\n",
    "        if (self.data[self.target].nunique() == 1):\n",
    "            self.node.setClassification(self.data[self.target].iloc[0])\n",
    "            #print(\"basecase1\")-----------------------------------------------------------------\n",
    "            return \n",
    "        # 2) out of discriptive features -> leaf node with majority of target values\n",
    "        if (not self.attributes):\n",
    "            self.node.setClassification(self.classify())\n",
    "            #print(\"basecase2\")-----------------------------------------------------------------\n",
    "            return\n",
    "        # 3) no instances left in dataset -> take majority of parent node\n",
    "        if (self.data is None):\n",
    "            parent = self.node.getParent()\n",
    "            self.node.setClassification(parent.getClassification())\n",
    "            #print(\"basecase3\")-----------------------------------------------------------------\n",
    "            return\n",
    "        # 4) maximal recursion depth:\n",
    "        if self.recursion_depth >= self.max_recursion:\n",
    "            self.node.setClassification(self.classify())\n",
    "            #print(\"basecase4\")-----------------------------------------------------------------\n",
    "            return\n",
    "\n",
    "\n",
    "        # recursive case:\n",
    "        # choose attribute with highest explainatory power\n",
    "        #print(\"in recursion\")-----------------------------------------------------------------\n",
    "        #print(\"attributs: \", self.attributes)-----------------------------------------------------------------\n",
    "        attribute = self.chooseAttribute()\n",
    "        self.node.setAttribute(attribute)\n",
    "        self.node.setClassification(self.classify())\n",
    "\n",
    "        # split data according to attribute\n",
    "        attributeColumn = self.data.loc[:, attribute]\n",
    "        values = set(attributeColumn)\n",
    "        new_attributes = self.attributes\n",
    "        new_attributes.remove(attribute)\n",
    "        \n",
    "        recursion_depth = self.recursion_depth + 1\n",
    "\n",
    "        # chosen attribute is a continuous variable:\n",
    "        valueIsContinuous=False\n",
    "        if self.is_continuous(values):\n",
    "            #print(\"continuous\")-----------------------------------------------------------------\n",
    "            \n",
    "            targetColumn = self.data[self.target]\n",
    "            boundaries = self.getBoundaries(targetColumn, attributeColumn)\n",
    "            attributeColumn = self.replaceContinuous(boundaries, attributeColumn)\n",
    "            values = set(attributeColumn)\n",
    "            values = self.sortIntervals(values)\n",
    "            valueIsContinuous = True\n",
    "        \n",
    "        # create leaf node for each attribute value\n",
    "        for value in values:\n",
    "            # get the subset determined by the attribute value\n",
    "            mask = lambda aColumn, value :(row == value for row in aColumn) \n",
    "            subsetData = self.data.iloc[mask(attributeColumn, value),:]\n",
    "            # create a node in the tree\n",
    "            childNode = Node(parent=self.node, value=value, valueIsContinuous=valueIsContinuous, target=self.target)\n",
    "            self.node.setChild(childNode)\n",
    "            # train the node with the data subset\n",
    "            subset = train_data(data=subsetData, \n",
    "                                target=self.target, \n",
    "                                attributes=new_attributes, \n",
    "                                node=childNode, \n",
    "                                recursion_depth=recursion_depth, \n",
    "                                max_recursion = self.max_recursion)\n",
    "            subset.id3() # recursive call on all partitions\n",
    "            \n",
    "\n",
    "    def retrain(self, data):\n",
    "        self.data = data\n",
    "        self.id3()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fc2ed",
   "metadata": {},
   "source": [
    "### test_data Class\n",
    "\n",
    "to classify datapoints and test accuracy of tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "410057a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_data:\n",
    "    \n",
    "    def __init__(self, testData, target, node:Node):\n",
    "        self.testData = testData\n",
    "        self.target = target\n",
    "        self.rootNode = node\n",
    "\n",
    "        # check whether node is trained:\n",
    "        if node.getAttribute() is None:\n",
    "            raise TypeError(\"node has to be part of a trained Decisiontree\")\n",
    "\n",
    "    \n",
    "    def classify(self, datapoint, node):\n",
    "        \n",
    "        # get leaf node classification (basecase)\n",
    "        if node.isLeaf() == True:\n",
    "            return node.getClassification()\n",
    "\n",
    "        # traverse down the tree with the decision nodes (recursive case)\n",
    "        else:\n",
    "            attribute = node.getAttribute()\n",
    "            dataValue = datapoint.loc[attribute]\n",
    "            for child in node.getChildren():\n",
    "                cValue = child.getValue()\n",
    "                # for interval values\n",
    "                if child.valueIsContinuous:\n",
    "                    if dataValue >= cValue[0] and dataValue < cValue[1]:\n",
    "                        return self.classify(datapoint, child)\n",
    "                # for discrete values\n",
    "                elif cValue is dataValue:\n",
    "                    return self.classify(datapoint, child)\n",
    "        \n",
    "        # if there are no children with the right value at decision node, get current classification (base case)\n",
    "        return node.getClassification()\n",
    "        \n",
    "    def classifySet(self):\n",
    "        classes = []\n",
    "        #print(\"testData: \", self.testData)\n",
    "        for i in range(len(self.testData)):\n",
    "            datapoint = self.testData.iloc[i]\n",
    "            #print(\"datapoint: \", datapoint)\n",
    "            classes.append(self.classify(datapoint, self.rootNode))\n",
    "        return classes\n",
    "    \n",
    "    def accuracy(self):\n",
    "        classes = self.classifySet()\n",
    "        targets = self.testData[self.target]\n",
    "        \n",
    "        errors = []\n",
    "        for target, classification in zip(targets, classes):\n",
    "            #print(\"target: \", target , \" class: \", classification)\n",
    "            if target == classification:\n",
    "                errors.append(True)\n",
    "            else:\n",
    "                errors.append(False)\n",
    "                \n",
    "        return np.mean(errors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee8adfa",
   "metadata": {},
   "source": [
    "### prepare_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "601c2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data:pd.DataFrame, tratio = 0.1):\n",
    "    \n",
    "    # remove any Nans from Dataframe\n",
    "    data = data.dropna(how='any')\n",
    "    \n",
    "    # shuffle data\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # variables to return\n",
    "    testData = []\n",
    "    trainingData = []\n",
    "\n",
    "    # check whether tratio is smaller than 1\n",
    "    if tratio >= 1:\n",
    "        raise TypeError(\"tratio has to be smaller than 1\")\n",
    "    \n",
    "    # get length of dataframe\n",
    "    dataLength = data.shape[0]\n",
    "    # get chunck size:\n",
    "    chunkSize = int(dataLength * tratio) \n",
    "    # get number of chunks\n",
    "    nr_chunks = int(1/tratio)\n",
    "\n",
    "    # append data set to existing dataset\n",
    "    doubleData = data.append(data)\n",
    "\n",
    "    # itterate through doubleData, assigh a certain chunk to testing and training\n",
    "    for chunk in range(nr_chunks-1):\n",
    "        testData.append(doubleData.iloc[chunk: chunkSize*(chunk+1), :])\n",
    "        trainingData.append(doubleData.iloc[chunkSize*(chunk+1): chunkSize*(nr_chunks+chunk), :])\n",
    "    \n",
    "    return [testData, trainingData]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d939c",
   "metadata": {},
   "source": [
    "### training with pokemon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e31b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_8604\\238235554.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree:\n",
      " # :  smaller then 154.5\n",
      "     classification:  Generation  =  1\n",
      " # :  between 154.5 and 242.0\n",
      "     classification:  Generation  =  2\n",
      "Decision tree:\n",
      " # :  smaller then 152.0\n",
      "     classification:  Generation  =  1\n",
      " # :  between 152.0 and 243.5\n",
      "     classification:  Generation  =  2\n",
      "Decision tree:\n",
      " # :  smaller then 155.0\n",
      "     classification:  Generation  =  1\n",
      "Decision tree:\n",
      " # :  smaller then 156.5\n",
      "     classification:  Generation  =  1\n",
      "Decision tree:\n",
      " # :  smaller then 156.5\n",
      "     classification:  Generation  =  1\n",
      "Decision tree:\n",
      " # :  smaller then 156.0\n",
      "     classification:  Generation  =  1\n",
      " # :  between 156.0 and 253.0\n",
      "     classification:  Generation  =  2\n",
      "Decision tree:\n",
      " # :  smaller then 156.0\n",
      "     classification:  Generation  =  1\n",
      " # :  between 156.0 and 253.5\n",
      "     classification:  Generation  =  2\n",
      "Decision tree:\n",
      " # :  smaller then 156.0\n",
      "     classification:  Generation  =  1\n",
      " # :  between 156.0 and 253.5\n",
      "     classification:  Generation  =  2\n",
      "Decision tree:\n",
      " # :  smaller then 156.0\n",
      "     classification:  Generation  =  1\n",
      " # :  between 156.0 and 253.5\n",
      "     classification:  Generation  =  2\n"
     ]
    }
   ],
   "source": [
    "# 1. load data\n",
    "data = pd.read_csv(\"data/pokemon_no_duplicates.csv\")\n",
    "\n",
    "# 2. prepare data\n",
    "data = prepare_data(data)\n",
    "\n",
    "# 3. choose the target value\n",
    "target = \"Generation\"\n",
    "\n",
    "# 4. train a tree for each chunk of the training set\n",
    "for trainingSet in data[0]:\n",
    "    \n",
    "    attributes = list(trainingSet.columns)\n",
    "    attributes.remove(target)\n",
    "    \n",
    "    rootNode = Node()\n",
    "    decisionTree = train_data(data=trainingSet, target=target, attributes=attributes, node=rootNode, max_recursion = 5)\n",
    "    decisionTree.id3()\n",
    "\n",
    "    rootNode.printTree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a440a1",
   "metadata": {},
   "source": [
    "### training with seaborn toy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384b9e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
      "       'alive', 'alone'],\n",
      "      dtype='object')\n",
      "     survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
      "1           1       1  female  38.0      1      0  71.2833        C  First   \n",
      "3           1       1  female  35.0      1      0  53.1000        S  First   \n",
      "6           0       1    male  54.0      0      0  51.8625        S  First   \n",
      "10          1       3  female   4.0      1      1  16.7000        S  Third   \n",
      "11          1       1  female  58.0      0      0  26.5500        S  First   \n",
      "..        ...     ...     ...   ...    ...    ...      ...      ...    ...   \n",
      "871         1       1  female  47.0      1      1  52.5542        S  First   \n",
      "872         0       1    male  33.0      0      0   5.0000        S  First   \n",
      "879         1       1  female  56.0      0      1  83.1583        C  First   \n",
      "887         1       1  female  19.0      0      0  30.0000        S  First   \n",
      "889         1       1    male  26.0      0      0  30.0000        C  First   \n",
      "\n",
      "       who  adult_male deck  embark_town alive  alone  \n",
      "1    woman       False    C    Cherbourg   yes  False  \n",
      "3    woman       False    C  Southampton   yes  False  \n",
      "6      man        True    E  Southampton    no   True  \n",
      "10   child       False    G  Southampton   yes  False  \n",
      "11   woman       False    C  Southampton   yes   True  \n",
      "..     ...         ...  ...          ...   ...    ...  \n",
      "871  woman       False    D  Southampton   yes  False  \n",
      "872    man        True    B  Southampton    no   True  \n",
      "879  woman       False    C    Cherbourg   yes  False  \n",
      "887  woman       False    B  Southampton   yes   True  \n",
      "889    man        True    C    Cherbourg   yes   True  \n",
      "\n",
      "[182 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# check out available dataset form seaborn\n",
    "\n",
    "# datasets from seaborn:\n",
    "# ['anagrams', 'anscombe', 'attention', 'brain_networks', 'car_crashes',\n",
    "#'diamonds', 'dots', 'exercise', 'flights', 'fmri', 'gammas', 'geyser',\n",
    "#'iris', 'mpg', 'penguins', 'planets', 'taxis', 'tips', 'titanic']\n",
    "\n",
    "data = sns.load_dataset(\"titanic\")\n",
    "data = data.dropna(how = \"any\")\n",
    "print(data.columns)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1188108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree:\n",
      " flipper_length_mm :  smaller then 195.0\n",
      "     classification:  species  =  Adelie\n",
      "Decision tree:\n",
      " island :  Dream\n",
      "     bill_length_mm :  smaller then 40.3\n",
      "         classification:  species  =  Adelie\n",
      " island :  Biscoe\n",
      "     flipper_length_mm :  smaller then 200.0\n",
      "         classification:  species  =  Adelie\n",
      "     flipper_length_mm :  bigger then 200.0\n",
      "         classification:  species  =  Gentoo\n",
      " island :  Torgersen\n",
      "     classification:  species  =  Adelie\n",
      "Decision tree:\n",
      " island :  Dream\n",
      "     bill_length_mm :  smaller then 40.55\n",
      "         classification:  species  =  Adelie\n",
      "     bill_length_mm :  between 40.55 and 41.0\n",
      "         classification:  species  =  Chinstrap\n",
      " island :  Biscoe\n",
      "     bill_depth_mm :  smaller then 16.9\n",
      "         classification:  species  =  Gentoo\n",
      "     bill_depth_mm :  bigger then 16.9\n",
      "         classification:  species  =  Adelie\n",
      " island :  Torgersen\n",
      "     classification:  species  =  Adelie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_8604\\238235554.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree:\n",
      " island :  Dream\n",
      "     bill_length_mm :  smaller then 40.55\n",
      "         classification:  species  =  Adelie\n",
      "     bill_length_mm :  between 40.55 and 41.0\n",
      "         classification:  species  =  Chinstrap\n",
      " island :  Biscoe\n",
      "     flipper_length_mm :  smaller then 203.0\n",
      "         classification:  species  =  Adelie\n",
      " island :  Torgersen\n",
      "     classification:  species  =  Adelie\n",
      "Decision tree:\n",
      " island :  Dream\n",
      "     bill_length_mm :  smaller then 40.9\n",
      "         classification:  species  =  Adelie\n",
      "     bill_length_mm :  between 40.9 and 41.0\n",
      "         sex :  Female\n",
      "             classification:  species  =  Chinstrap\n",
      "         sex :  Male\n",
      "             classification:  species  =  Adelie\n",
      " island :  Biscoe\n",
      "     flipper_length_mm :  smaller then 203.0\n",
      "         classification:  species  =  Adelie\n",
      " island :  Torgersen\n",
      "     classification:  species  =  Adelie\n",
      "Decision tree:\n",
      " island :  Dream\n",
      "     bill_length_mm :  smaller then 40.9\n",
      "         classification:  species  =  Adelie\n",
      "     bill_length_mm :  between 40.9 and 41.0\n",
      "         sex :  Female\n",
      "             classification:  species  =  Chinstrap\n",
      "         sex :  Male\n",
      "             classification:  species  =  Adelie\n",
      " island :  Biscoe\n",
      "     flipper_length_mm :  smaller then 201.0\n",
      "         classification:  species  =  Adelie\n",
      " island :  Torgersen\n",
      "     classification:  species  =  Adelie\n",
      "Decision tree:\n",
      " island :  Dream\n",
      "     bill_length_mm :  smaller then 40.9\n",
      "         classification:  species  =  Adelie\n",
      "     bill_length_mm :  between 40.9 and 41.0\n",
      "         sex :  Female\n",
      "             classification:  species  =  Chinstrap\n",
      "         sex :  Male\n",
      "             classification:  species  =  Adelie\n",
      " island :  Biscoe\n",
      "     flipper_length_mm :  smaller then 201.0\n",
      "         classification:  species  =  Adelie\n",
      " island :  Torgersen\n",
      "     classification:  species  =  Adelie\n",
      "Decision tree:\n",
      " island :  Dream\n",
      "     bill_length_mm :  smaller then 40.9\n",
      "         classification:  species  =  Adelie\n",
      "     bill_length_mm :  between 40.9 and 41.0\n",
      "         sex :  Female\n",
      "             classification:  species  =  Chinstrap\n",
      "         sex :  Male\n",
      "             classification:  species  =  Adelie\n",
      " island :  Biscoe\n",
      "     flipper_length_mm :  smaller then 201.5\n",
      "         classification:  species  =  Adelie\n",
      " island :  Torgersen\n",
      "     classification:  species  =  Adelie\n",
      "Decision tree:\n",
      " island :  Dream\n",
      "     bill_length_mm :  smaller then 40.9\n",
      "         classification:  species  =  Adelie\n",
      "     bill_length_mm :  between 40.9 and 41.0\n",
      "         sex :  Female\n",
      "             classification:  species  =  Chinstrap\n",
      "         sex :  Male\n",
      "             classification:  species  =  Adelie\n",
      " island :  Biscoe\n",
      "     flipper_length_mm :  smaller then 201.5\n",
      "         classification:  species  =  Adelie\n",
      " island :  Torgersen\n",
      "     classification:  species  =  Adelie\n",
      "0.4208754208754209\n",
      "0.7845117845117845\n",
      "0.7474747474747475\n",
      "0.7777777777777778\n",
      "0.797979797979798\n",
      "0.9730639730639731\n",
      "0.9663299663299664\n",
      "0.9730639730639731\n",
      "0.9663299663299664\n"
     ]
    }
   ],
   "source": [
    "data = sns.load_dataset(\"penguins\")\n",
    "\n",
    "# 2. prepare data\n",
    "data = prepare_data(data)\n",
    "\n",
    "# 3. choose the target value\n",
    "target = \"species\"\n",
    "\n",
    "# 4. train a tree for each chunk of the training set\n",
    "decisionTrees = []\n",
    "for trainingSet in data[0]:\n",
    "    \n",
    "    attributes = list(trainingSet.columns)\n",
    "    attributes.remove(target)\n",
    "    \n",
    "    rootNode = Node()\n",
    "    decisionTree = train_data(data=trainingSet, target=target, attributes=attributes, node=rootNode, max_recursion = 10)\n",
    "    decisionTree.id3()\n",
    "    decisionTrees.append(rootNode)\n",
    "    rootNode.printTree()\n",
    "\n",
    "for testingSet, tree in zip(data[1], decisionTrees):\n",
    "    \n",
    "    testData = test_data(testingSet, target, tree)\n",
    "    print(testData.accuracy())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7857dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_8604\\238235554.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n",
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_8604\\238235554.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n",
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_8604\\238235554.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n",
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_8604\\238235554.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n",
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_8604\\238235554.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n",
      "C:\\Users\\erna\\AppData\\Local\\Temp\\ipykernel_8604\\238235554.py:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  doubleData = data.append(data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6746691289627578, 0.676954732510288, 0.673611111111111, 0.6574074074074074, 0.6666666666666666, 0.7032967032967034]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. load data\n",
    "data = sns.load_dataset(\"titanic\")\n",
    "data = data.drop(\"alive\", axis=1)\n",
    "\n",
    "accuraciesAll = []\n",
    "for ratio in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    # 2. prepare data\n",
    "    dataPrepared = prepare_data(data = data, tratio=ratio)\n",
    "\n",
    "    # 3. choose the target value\n",
    "    target = \"survived\"\n",
    "\n",
    "    accuracies = []\n",
    "    decisionTrees = []\n",
    "    for trainingSet in dataPrepared[0]:\n",
    "\n",
    "        attributes = list(trainingSet.columns)\n",
    "        attributes.remove(target)\n",
    "\n",
    "        rootNode = Node()\n",
    "        decisionTree = train_data(data=trainingSet, target=target, attributes=attributes, node=rootNode, max_recursion = 10)\n",
    "        decisionTree.id3()\n",
    "        decisionTrees.append(rootNode)\n",
    "        #rootNode.printTree()\n",
    "\n",
    "    for testingSet, tree in zip(dataPrepared[1], decisionTrees):\n",
    "\n",
    "        testData = test_data(testingSet, target, tree)\n",
    "        accuracies.append(testData.accuracy())\n",
    "        \n",
    "    accuraciesAll.append(np.mean(accuracies))\n",
    "print(accuraciesAll) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bcb29d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28a41b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
